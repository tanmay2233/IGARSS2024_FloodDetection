{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vP3_bxnJc-lH",
        "outputId": "c77a471b-b09d-44f5-d7de-16b1069d9bdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n"
      ],
      "metadata": {
        "id": "AmORJYxDeBKr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.psi(F.relu(g1 + x1))\n",
        "        return x * psi\n",
        "\n",
        "\n",
        "class AttentionUNet(nn.Module):\n",
        "    def __init__(self, in_channels=6, out_channels=1, dropout_prob=0.3):\n",
        "        super(AttentionUNet, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            self.conv_block(in_channels, 64),\n",
        "            self.conv_block(64, 128),\n",
        "            self.conv_block(128, 256),\n",
        "            self.conv_block(256, 512)\n",
        "        )\n",
        "\n",
        "        self.center = self.conv_block(512, 1024)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            self.up_conv(1024, 512),\n",
        "            AttentionBlock(512, 512, 256),\n",
        "            self.conv_block(512, 256),\n",
        "\n",
        "            self.up_conv(256, 256),\n",
        "            AttentionBlock(256, 256, 128),\n",
        "            self.conv_block(256, 128),\n",
        "\n",
        "            self.up_conv(128, 128),\n",
        "            AttentionBlock(128, 128, 64),\n",
        "            self.conv_block(128, 64)\n",
        "        )\n",
        "\n",
        "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "        # MC Dropout layers\n",
        "        self.dropout = nn.Dropout2d(p=dropout_prob)\n",
        "\n",
        "    def conv_block(self, in_c, out_c):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def up_conv(self, in_c, out_c):\n",
        "        return nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.encoder[0](x)\n",
        "        e2 = self.encoder[1](e1)\n",
        "        e3 = self.encoder[2](e2)\n",
        "        e4 = self.encoder[3](e3)\n",
        "\n",
        "        center = self.center(e4)\n",
        "        center = self.dropout(center)  # MC Dropout\n",
        "\n",
        "        d4 = self.decoder[0](center)\n",
        "        d4 = torch.cat((d4, e4), dim=1)\n",
        "        d4 = self.decoder[2](d4)\n",
        "\n",
        "        d3 = self.decoder[3](d4)\n",
        "        d3 = torch.cat((d3, e3), dim=1)\n",
        "        d3 = self.decoder[5](d3)\n",
        "\n",
        "        d2 = self.decoder[6](d3)\n",
        "        d2 = torch.cat((d2, e2), dim=1)\n",
        "        d2 = self.decoder[8](d2)\n",
        "\n",
        "        d1 = torch.cat((d2, e1), dim=1)\n",
        "        d1 = self.decoder[9](d1)\n",
        "\n",
        "        out = self.final(d1)\n",
        "        return torch.sigmoid(out)\n"
      ],
      "metadata": {
        "id": "KBsc6vr4ePSV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from glob import glob\n",
        "import os\n",
        "\n",
        "class SARFloodDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_paths = sorted(glob(os.path.join(image_dir, \"*.tif\")))\n",
        "        self.mask_paths = sorted(glob(os.path.join(mask_dir, \"*.png\")))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load SAR image (TIF)\n",
        "        with rasterio.open(self.image_paths[idx]) as src:\n",
        "            img = src.read()  # (C, H, W) format\n",
        "\n",
        "        img = np.moveaxis(img, 0, -1)  # Convert to (H, W, C) for Albumentations\n",
        "\n",
        "        # Load mask (PNG)\n",
        "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)  # (H, W)\n",
        "        mask = (mask > 127).astype(np.float32)  # Convert to binary mask\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=img, mask=mask)\n",
        "            img, mask = augmented['image'], augmented['mask']\n",
        "\n",
        "        return img, mask.float()\n"
      ],
      "metadata": {
        "id": "jIO5zAtfeQlm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=0.25):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
        "        p_t = inputs * targets + (1 - inputs) * (1 - targets)\n",
        "        loss = self.alpha * (1 - p_t) ** self.gamma * BCE\n",
        "        return loss.mean()\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        intersection = (inputs * targets).sum()\n",
        "        return 1 - ((2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth))\n",
        "\n",
        "def combined_loss(inputs, targets):\n",
        "    return FocalLoss()(inputs, targets) + DiceLoss()(inputs, targets)\n"
      ],
      "metadata": {
        "id": "WnvfE-06eYO9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, optimizer, num_epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for images, masks in tqdm(dataloader):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = combined_loss(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader)}\")\n"
      ],
      "metadata": {
        "id": "ovvXgY3BeaEK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_uncertainty(f_model, images, n_iter=10):\n",
        "    f_model.train()  # Keep dropout layers active\n",
        "    preds = torch.stack([f_model(images) for _ in range(n_iter)], dim=0)\n",
        "    mean_pred = preds.mean(dim=0)\n",
        "    uncertainty = preds.var(dim=0)\n",
        "    return mean_pred, uncertainty\n"
      ],
      "metadata": {
        "id": "QNoEP_u5egQJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(512, 512),\n",
        "    A.RandomRotate90(),\n",
        "    A.HorizontalFlip(p=0.5),  # Use this instead of A.Flip()\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.Normalize(mean=[0], std=[1]),  # Adjust SAR normalization as needed\n",
        "    ToTensorV2()\n",
        "])\n"
      ],
      "metadata": {
        "id": "iSwN_U0egCSx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Auto-detect device (use CPU if no GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize dataset and DataLoader\n",
        "train_dataset = SARFloodDataset(\n",
        "    image_dir=\"/content/drive/MyDrive/train/images\",\n",
        "    mask_dir=\"/content/drive/MyDrive/train/labels\",\n",
        "    transform=train_transform\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "# Initialize model & optimizer on correct device\n",
        "model = AttentionUNet(in_channels=6).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Train model\n",
        "train_model(model, train_loader, optimizer, num_epochs=10)\n"
      ],
      "metadata": {
        "id": "gthHZr0of9df",
        "outputId": "83c6cfb3-8216-47d4-c4ba-c927c7737fb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/17 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/rasterio/__init__.py:356: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
            "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colaboratory",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}