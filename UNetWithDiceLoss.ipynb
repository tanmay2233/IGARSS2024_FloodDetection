{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"gpuType":"T4","name":"Welcome to Colaboratory","provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10756922,"sourceType":"datasetVersion","datasetId":6672060}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"pip install rasterio","metadata":{"execution":{"iopub.execute_input":"2025-02-19T13:52:53.314168Z","iopub.status.busy":"2025-02-19T13:52:53.313813Z","iopub.status.idle":"2025-02-19T13:52:56.707494Z","shell.execute_reply":"2025-02-19T13:52:56.706233Z","shell.execute_reply.started":"2025-02-19T13:52:53.314139Z"},"trusted":true}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2","metadata":{"id":"AmORJYxDeBKr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvBlock(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.conv(x)\n\nclass UpConvBlock(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super().__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.up(x)\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, f_g, f_l, f_int):\n        super().__init__()\n        self.w_g = nn.Sequential(\n            nn.Conv2d(f_g, f_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(f_int)\n        )\n        self.w_x = nn.Sequential(\n            nn.Conv2d(f_l, f_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(f_int)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(f_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, g, x):\n        g1 = self.w_g(g)\n        x1 = self.w_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        return x * psi\n\nimport torch.nn as nn\nimport torch\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=6, out_channels=1):\n        super().__init__() \n        \n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.conv1 = ConvBlock(ch_in=in_channels, ch_out=64)\n        self.conv2 = ConvBlock(ch_in=64, ch_out=128)\n        self.conv3 = ConvBlock(ch_in=128, ch_out=256)\n        self.conv4 = ConvBlock(ch_in=256, ch_out=512)\n        self.conv5 = ConvBlock(ch_in=512, ch_out=1024)\n        \n        self.up5 = UpConvBlock(ch_in=1024, ch_out=512)\n        self.att5 = AttentionBlock(f_g=512, f_l=512, f_int=256)\n        self.upconv5 = ConvBlock(ch_in=1024, ch_out=512)\n        \n        self.up4 = UpConvBlock(ch_in=512, ch_out=256)\n        self.att4 = AttentionBlock(f_g=256, f_l=256, f_int=128)\n        self.upconv4 = ConvBlock(ch_in=512, ch_out=256)\n        \n        self.up3 = UpConvBlock(ch_in=256, ch_out=128)\n        self.att3 = AttentionBlock(f_g=128, f_l=128, f_int=64)\n        self.upconv3 = ConvBlock(ch_in=256, ch_out=128)\n        \n        self.up2 = UpConvBlock(ch_in=128, ch_out=64)\n        self.att2 = AttentionBlock(f_g=64, f_l=64, f_int=32)\n        self.upconv2 = ConvBlock(ch_in=128, ch_out=64)\n        \n        self.conv_1x1 = nn.Conv2d(64, out_channels, kernel_size=1, stride=1, padding=0)\n        \n    def forward(self, x):\n        # Encoder\n        x1 = self.conv1(x)\n        x2 = self.conv2(self.maxpool(x1))\n        x3 = self.conv3(self.maxpool(x2))\n        x4 = self.conv4(self.maxpool(x3))\n        x5 = self.conv5(self.maxpool(x4))\n        \n        # Decoder + Attention\n        d5 = self.upconv5(torch.cat((self.att5(self.up5(x5), x4), self.up5(x5)), dim=1))\n        d4 = self.upconv4(torch.cat((self.att4(self.up4(d5), x3), self.up4(d5)), dim=1))\n        d3 = self.upconv3(torch.cat((self.att3(self.up3(d4), x2), self.up3(d4)), dim=1))\n        d2 = self.upconv2(torch.cat((self.att2(self.up2(d3), x1), self.up2(d3)), dim=1))\n        \n        d1 = self.conv_1x1(d2)\n        \n        return torch.sigmoid(d1)  # Output probability map\n","metadata":{"id":"KBsc6vr4ePSV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nimport rasterio\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\nclass SARDEMDataset(Dataset):\n    def __init__(self, img_paths, label_paths, transform=None):\n        self.img_paths = img_paths\n        self.label_paths = label_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img = tiff.imread(self.img_paths[idx]).astype(np.float32)  # Convert to float32\n        label = plt.imread(self.label_paths[idx])\n\n        # Normalize label to 0 or 1\n        label = (label > 0.001).astype(np.uint8)\n\n        if self.transform:\n            img = self.transform(img)\n            label = torch.tensor(label, dtype=torch.float32).unsqueeze(0)  # Ensure float32\n        \n        return img, label\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T10:20:06.012105Z","iopub.execute_input":"2025-03-19T10:20:06.012358Z","iopub.status.idle":"2025-03-19T10:20:06.183770Z","shell.execute_reply.started":"2025-03-19T10:20:06.012338Z","shell.execute_reply":"2025-03-19T10:20:06.182841Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Paths\nimg_dir = \"/kaggle/input/images/images/images\"\nlabel_dir = \"/kaggle/input/images/labels/labels\"\n\n# List images and labels\nimg_list = sorted([f for f in os.listdir(img_dir) if f.endswith(\".tif\")])\nlabel_list = sorted([f for f in os.listdir(label_dir) if f.endswith(\".png\")])\n\n# Keep only matching pairs\nimg_paths = [os.path.join(img_dir, f) for f in img_list if f.replace(\".tif\", \".png\") in label_list]\nlabel_paths = [os.path.join(label_dir, f.replace(\".tif\", \".png\")) for f in img_list if f.replace(\".tif\", \".png\") in label_list]\n\n# Train-test split\ntrain_img_paths, test_img_paths, train_label_paths, test_label_paths = train_test_split(img_paths, label_paths, test_size=0.2, random_state=42)\n\n# Data Augmentation & Transformation\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n# Create Datasets\ntrain_dataset = SARDEMDataset(train_img_paths, train_label_paths, transform=transform)\ntest_dataset = SARDEMDataset(test_img_paths, test_label_paths, transform=transform)\n\n# Data Loaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T10:20:06.184962Z","iopub.execute_input":"2025-03-19T10:20:06.185317Z","iopub.status.idle":"2025-03-19T10:20:06.671432Z","shell.execute_reply.started":"2025-03-19T10:20:06.185295Z","shell.execute_reply":"2025-03-19T10:20:06.670670Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# import torch.optim as optim\n\n# def dice_loss(pred, target, smooth=1e-5):\n#     pred = torch.sigmoid(pred)\n#     intersection = (pred * target).sum()\n#     dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n#     return 1 - dice\n\n# class CombinedLoss(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.bce = nn.BCEWithLogitsLoss()\n\n#     def forward(self, pred, target):\n#         return self.bce(pred, target) + dice_loss(pred, target)\n\n\n# # Device setup\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Model\n# model = UNet(in_channels=6, out_channels=1).to(device)\n\n# # Loss Function\n# criterion = CombinedLoss()  # Binary Cross Entropy for segmentation\n\n# # Optimizer\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T10:20:06.672313Z","iopub.execute_input":"2025-03-19T10:20:06.672861Z","iopub.status.idle":"2025-03-19T10:20:06.677366Z","shell.execute_reply.started":"2025-03-19T10:20:06.672794Z","shell.execute_reply":"2025-03-19T10:20:06.676340Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n# def train(model, dataloader, optimizer, criterion, device):\n#     model.train()\n#     total_loss = 0\n#     for img, label in dataloader:\n#         img, label = img.to(device), label.to(device)\n        \n#         optimizer.zero_grad()\n#         output = model(img)\n        \n#         loss = criterion(output, label)\n#         loss.backward()\n#         optimizer.step()\n        \n#         total_loss += loss.item()\n    \n#     avg_loss = total_loss / len(dataloader)\n#     print(f\"Train Loss: {avg_loss:.4f}\")\n\n# def evaluate(model, dataloader, device):\n#     model.eval()\n#     correct = 0\n#     total = 0\n#     with torch.no_grad():\n#         for img, label in dataloader:\n#             img, label = img.to(device), label.to(device)\n#             output = model(img)\n#             predicted = (output > 0.5).float()\n#             correct += (predicted == label).sum().item()\n#             total += label.numel()\n    \n#     accuracy = 100 * correct / total\n#     print(f\"Accuracy: {accuracy:.2f}%\")\n\n# # Train the Model\n# epochs = 10\n# for epoch in range(epochs):\n#     print(f\"Epoch {epoch+1}/{epochs}\")\n#     train(model, train_loader, optimizer, criterion, device)\n#     evaluate(model, test_loader, device)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T10:20:06.678328Z","iopub.execute_input":"2025-03-19T10:20:06.678617Z","iopub.status.idle":"2025-03-19T10:20:06.691629Z","shell.execute_reply.started":"2025-03-19T10:20:06.678587Z","shell.execute_reply":"2025-03-19T10:20:06.690802Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Import necessary modules\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport tifffile as tiff\nimport numpy as np\n\n# Define the device (use GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize your UNet model\nunet = UNet(in_channels=6, out_channels=1)  # Adjust input channels for SAR, DEM, LULC, etc.\nunet.to(device)\n\n\ndef dice_coef_metric(inputs, target):\n    intersection = 2.0 * (target * inputs).sum()\n    union = target.sum() + inputs.sum()\n    if target.sum() == 0 and inputs.sum() == 0:\n        return 1.0  # If both are empty, perfect match\n    return intersection / union\n\nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1.0):\n        # Apply sigmoid activation\n        inputs = torch.sigmoid(inputs)       \n        \n        # Flatten tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()\n        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n        \n        return 1 - dice  # Dice loss is (1 - dice coefficient)\n\ndef train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler=None, num_epochs=50):\n    print(f\"[INFO] Training Model: {model_name}\")\n    \n    loss_history = []\n    train_history = []\n    val_history = []\n    \n    bce_loss_fn = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss\n    \n    for epoch in range(num_epochs):\n        model.train()\n        \n        epoch_losses = []\n        epoch_dice = []\n        \n        for i_step, (data, target) in enumerate(tqdm(train_loader)):\n            data, target = data.to(device), target.to(device)\n            \n            outputs = model(data)\n            \n            # Convert outputs to binary mask using threshold\n            out_cut = torch.sigmoid(outputs)  # Apply sigmoid activation\n            out_cut = torch.where(out_cut < 0.5, torch.tensor(0.0, device=device), torch.tensor(1.0, device=device))\n            \n            # Compute Dice coefficient\n            train_dice = dice_coef_metric(out_cut, target)\n            \n            # Compute BCE + Dice Loss\n            bce_loss = bce_loss_fn(outputs, target)\n            dice_loss = train_loss(outputs, target)\n            loss = bce_loss + dice_loss  # Combined loss\n            \n            epoch_losses.append(loss.item())\n            epoch_dice.append(train_dice)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n        val_dice = compute_iou(model, val_loader)\n        \n        # Learning rate scheduling (if applicable)\n        if lr_scheduler:\n            lr_scheduler.step()\n        \n        loss_history.append(np.mean(epoch_losses))\n        train_history.append(np.mean([x.cpu().numpy() for x in epoch_dice]))\n        val_history.append(val_dice)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n        print(f\"Train Loss: {np.mean(epoch_losses):.4f}, Train Dice: {np.mean([x.cpu().numpy() for x in epoch_dice]):.4f}, Val Dice: {val_dice:.4f}\")\n\n    \n    return loss_history, train_history, val_history\n\ndef compute_iou(model, loader, threshold=0.5):\n    total_iou = 0\n    count = 0\n    \n    with torch.no_grad():\n        model.eval()\n\n        for i_step, (data, target) in enumerate(loader):\n            data, target = data.to(device), target.to(device)\n            \n            outputs = model(data)\n            out_cut = torch.sigmoid(outputs)\n            out_cut = torch.where(out_cut < threshold, torch.tensor(0.0, device=device), torch.tensor(1.0, device=device))\n            \n            iou = dice_coef_metric(out_cut, target)\n            total_iou += iou\n            count += 1\n\n    return total_iou / count if count > 0 else 0\n\n# Define optimizer\nopt = torch.optim.Adamax(unet.parameters(), lr=1e-3)\n\n# Train Model\nnum_epochs = 5\naun_lh, aun_th, aun_vh = train_model(\"UNet\", unet, train_loader, test_loader, DiceLoss(), opt, None, num_epochs)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T10:20:06.692364Z","iopub.execute_input":"2025-03-19T10:20:06.692620Z","execution_failed":"2025-03-19T10:20:17.971Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[INFO] Training Model: Attention UNet\n","output_type":"stream"},{"name":"stderr","text":"  4%|‚ñç         | 13/326 [00:10<04:07,  1.26it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"torch.save(unet.state_dict(), \"flood_segmentation_model.pth\")\nprint(\"Model saved successfully!\")\n","metadata":{"execution":{"iopub.execute_input":"2025-02-19T15:20:07.262629Z","iopub.status.busy":"2025-02-19T15:20:07.262251Z","iopub.status.idle":"2025-02-19T15:20:07.611138Z","shell.execute_reply":"2025-02-19T15:20:07.610343Z","shell.execute_reply.started":"2025-02-19T15:20:07.262603Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved successfully!\n"]}],"execution_count":148},{"cell_type":"code","source":"# Load the trained model\nmodel = UNet(6, 1)  # Use the correct model class\nmodel.load_state_dict(torch.load(\"flood_segmentation_model.pth\"))\nmodel.to(device)\nmodel.eval()  # Set to evaluation mode\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tifffile as tiff\nimport torch\nimport numpy as np\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define image path\nimage_path = \"/kaggle/input/images/images/images/4.tif\"\n\n# Load and preprocess the image using tifffile\nimage = tiff.imread(image_path)\n\n# Normalize and convert to tensor\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # Convert to tensor\n])\n\nimage = transform(image).float()  # Convert to float32\nimage = image.unsqueeze(0)  # Add batch dimension\nimage = image.to(device)\n\nprint(f\"Image dtype: {image.dtype}\")  # Should print `torch.float32`\n\n# Run inference\nmodel.eval()\nwith torch.no_grad():\n    output = model(image)\n\n# Convert output to binary mask\npred_mask = output.sigmoid().cpu().numpy().squeeze()\npred_mask = (pred_mask > 0.6).astype(np.uint8)  # Binary (0 or 1)\n\n# Load ground truth (assuming it's a PNG image)\nlabel_path = \"/kaggle/input/images/labels/labels/4.png\"\ngt_mask = plt.imread(label_path)\n\n# Normalize GT Mask to Binary (0 or 1)\ngt_mask = (gt_mask > 0.001).astype(np.uint8)\n\n\n# Visualize prediction vs ground truth\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nax[0].imshow(gt_mask, cmap=\"gray\")\nax[0].set_title(\"Ground Truth\")\nax[0].axis(\"off\")\n\nax[1].imshow(pred_mask, cmap=\"gray\")\nax[1].set_title(\"Predicted Mask\")\nax[1].axis(\"off\")\n\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-02-19T15:42:18.415000Z","iopub.status.busy":"2025-02-19T15:42:18.414658Z","iopub.status.idle":"2025-02-19T15:42:18.796402Z","shell.execute_reply":"2025-02-19T15:42:18.795658Z","shell.execute_reply.started":"2025-02-19T15:42:18.414978Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Image dtype: torch.float32\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 1000x500 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"execution_count":160},{"cell_type":"code","source":"# ----------------------- Compute Training Accuracy -----------------------\ntrain_correct = 0\ntrain_total = 0\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        pred_masks = (outputs.sigmoid() > 0.6).float()\n\n        train_correct += (pred_masks == labels).sum().item()\n        train_total += labels.numel()\n\ntrain_accuracy = train_correct / train_total\nprint(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n\n# ---------------------- Evaluate Model on Test Data ----------------------\ntest_preds = []\ntest_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        pred_masks = (outputs.sigmoid() > 0.6).cpu().numpy().astype(np.uint8)\n        labels = labels.cpu().numpy().astype(np.uint8)\n\n        test_preds.extend(pred_masks.flatten())\n        test_labels.extend(labels.flatten())\n\n# Compute Accuracy & F1 Score\ntest_accuracy = accuracy_score(test_labels, test_preds)\ntest_f1 = f1_score(test_labels, test_preds)\n\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\nprint(f\"Test F1 Score: {test_f1:.4f}\")\n","metadata":{"execution":{"iopub.execute_input":"2025-02-19T15:42:30.747304Z","iopub.status.busy":"2025-02-19T15:42:30.746985Z","iopub.status.idle":"2025-02-19T15:49:14.673964Z","shell.execute_reply":"2025-02-19T15:49:14.672995Z","shell.execute_reply.started":"2025-02-19T15:42:30.747280Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Accuracy: 97.44%\n","Test Accuracy: 97.28%\n","Test F1 Score: 0.8163\n"]}],"execution_count":161},{"cell_type":"code","source":"pred_mask, gt_mask","metadata":{"execution":{"iopub.execute_input":"2025-02-19T15:49:14.675318Z","iopub.status.busy":"2025-02-19T15:49:14.675071Z","iopub.status.idle":"2025-02-19T15:49:14.680907Z","shell.execute_reply":"2025-02-19T15:49:14.680228Z","shell.execute_reply.started":"2025-02-19T15:49:14.675298Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(array([[0, 0, 0, ..., 0, 0, 0],\n","        [0, 0, 0, ..., 0, 0, 0],\n","        [0, 0, 0, ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0, ..., 0, 0, 0],\n","        [0, 0, 0, ..., 0, 0, 0],\n","        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n"," array([[0, 0, 0, ..., 0, 0, 0],\n","        [0, 0, 0, ..., 0, 0, 0],\n","        [0, 0, 0, ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0, ..., 0, 0, 0],\n","        [0, 0, 0, ..., 0, 0, 0],\n","        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8))"]},"execution_count":162,"metadata":{},"output_type":"execute_result"}],"execution_count":162}]}