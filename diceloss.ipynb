{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","name":"Welcome to Colaboratory","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10756922,"sourceType":"datasetVersion","datasetId":6672060},{"sourceId":292352,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":250437,"modelId":271930}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T11:46:40.579576Z","iopub.execute_input":"2025-03-19T11:46:40.579916Z","iopub.status.idle":"2025-03-19T11:46:49.215786Z","shell.execute_reply.started":"2025-03-19T11:46:40.579883Z","shell.execute_reply":"2025-03-19T11:46:49.214777Z"},"id":"AmORJYxDeBKr","trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install rasterio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvBlock(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.conv(x)\n\nclass UpConvBlock(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super().__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.up(x)\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, f_g, f_l, f_int):\n        super().__init__()\n        self.w_g = nn.Sequential(\n            nn.Conv2d(f_g, f_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(f_int)\n        )\n        self.w_x = nn.Sequential(\n            nn.Conv2d(f_l, f_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(f_int)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(f_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, g, x):\n        g1 = self.w_g(g)\n        x1 = self.w_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        return x * psi\n\nimport torch.nn as nn\nimport torch\n\nclass AttentionUNet(nn.Module):\n    def __init__(self, in_channels=6, out_channels=1):\n        super().__init__() \n        \n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.conv1 = ConvBlock(ch_in=in_channels, ch_out=64)\n        self.conv2 = ConvBlock(ch_in=64, ch_out=128)\n        self.conv3 = ConvBlock(ch_in=128, ch_out=256)\n        self.conv4 = ConvBlock(ch_in=256, ch_out=512)\n        self.conv5 = ConvBlock(ch_in=512, ch_out=1024)\n        \n        self.up5 = UpConvBlock(ch_in=1024, ch_out=512)\n        self.att5 = AttentionBlock(f_g=512, f_l=512, f_int=256)\n        self.upconv5 = ConvBlock(ch_in=1024, ch_out=512)\n        \n        self.up4 = UpConvBlock(ch_in=512, ch_out=256)\n        self.att4 = AttentionBlock(f_g=256, f_l=256, f_int=128)\n        self.upconv4 = ConvBlock(ch_in=512, ch_out=256)\n        \n        self.up3 = UpConvBlock(ch_in=256, ch_out=128)\n        self.att3 = AttentionBlock(f_g=128, f_l=128, f_int=64)\n        self.upconv3 = ConvBlock(ch_in=256, ch_out=128)\n        \n        self.up2 = UpConvBlock(ch_in=128, ch_out=64)\n        self.att2 = AttentionBlock(f_g=64, f_l=64, f_int=32)\n        self.upconv2 = ConvBlock(ch_in=128, ch_out=64)\n        \n        self.conv_1x1 = nn.Conv2d(64, out_channels, kernel_size=1, stride=1, padding=0)\n        \n    def forward(self, x):\n        # Encoder\n        x1 = self.conv1(x)\n        x2 = self.conv2(self.maxpool(x1))\n        x3 = self.conv3(self.maxpool(x2))\n        x4 = self.conv4(self.maxpool(x3))\n        x5 = self.conv5(self.maxpool(x4))\n        \n        # Decoder + Attention\n        d5 = self.upconv5(torch.cat((self.att5(self.up5(x5), x4), self.up5(x5)), dim=1))\n        d4 = self.upconv4(torch.cat((self.att4(self.up4(d5), x3), self.up4(d5)), dim=1))\n        d3 = self.upconv3(torch.cat((self.att3(self.up3(d4), x2), self.up3(d4)), dim=1))\n        d2 = self.upconv2(torch.cat((self.att2(self.up2(d3), x1), self.up2(d3)), dim=1))\n        \n        d1 = self.conv_1x1(d2)\n        \n        return torch.sigmoid(d1)  # Output probability map\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T11:46:54.242992Z","iopub.execute_input":"2025-03-19T11:46:54.243342Z","iopub.status.idle":"2025-03-19T11:46:54.260802Z","shell.execute_reply.started":"2025-03-19T11:46:54.243314Z","shell.execute_reply":"2025-03-19T11:46:54.259739Z"},"id":"KBsc6vr4ePSV","trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nimport rasterio\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\nclass SARDEMDataset(Dataset):\n    def __init__(self, img_paths, label_paths, transform=None):\n        self.img_paths = img_paths\n        self.label_paths = label_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img = tiff.imread(self.img_paths[idx]).astype(np.float32)  # Convert to float32\n        label = plt.imread(self.label_paths[idx])\n\n        # Normalize label to 0 or 1\n        label = (label > 0.001).astype(np.uint8)\n\n        if self.transform:\n            img = self.transform(img)\n            label = torch.tensor(label, dtype=torch.float32).unsqueeze(0)  # Ensure float32\n        \n        return img, label\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T11:47:28.438606Z","iopub.execute_input":"2025-03-19T11:47:28.438985Z","iopub.status.idle":"2025-03-19T11:47:29.146983Z","shell.execute_reply.started":"2025-03-19T11:47:28.438943Z","shell.execute_reply":"2025-03-19T11:47:29.146227Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Paths\nimg_dir = \"/kaggle/input/images/images/images\"\nlabel_dir = \"/kaggle/input/images/labels/labels\"\n\n# List images and labels\nimg_list = sorted([f for f in os.listdir(img_dir) if f.endswith(\".tif\")])\nlabel_list = sorted([f for f in os.listdir(label_dir) if f.endswith(\".png\")])\n\n# Keep only matching pairs\nimg_paths = [os.path.join(img_dir, f) for f in img_list if f.replace(\".tif\", \".png\") in label_list]\nlabel_paths = [os.path.join(label_dir, f.replace(\".tif\", \".png\")) for f in img_list if f.replace(\".tif\", \".png\") in label_list]\n\n# Train-test split\ntrain_img_paths, test_img_paths, train_label_paths, test_label_paths = train_test_split(img_paths, label_paths, test_size=0.2, random_state=42)\n\n# Data Augmentation & Transformation\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n# Create Datasets\ntrain_dataset = SARDEMDataset(train_img_paths, train_label_paths, transform=transform)\ntest_dataset = SARDEMDataset(test_img_paths, test_label_paths, transform=transform)\n\n# Data Loaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T11:47:32.308329Z","iopub.execute_input":"2025-03-19T11:47:32.309075Z","iopub.status.idle":"2025-03-19T11:47:32.822369Z","shell.execute_reply.started":"2025-03-19T11:47:32.309044Z","shell.execute_reply":"2025-03-19T11:47:32.821402Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# import torch.optim as optim\n\n# def dice_loss(pred, target, smooth=1e-5):\n#     pred = torch.sigmoid(pred)\n#     intersection = (pred * target).sum()\n#     dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n#     return 1 - dice\n\n# class CombinedLoss(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.bce = nn.BCEWithLogitsLoss()\n\n#     def forward(self, pred, target):\n#         return self.bce(pred, target) + dice_loss(pred, target)\n\n\n# # Device setup\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Model\n# model = AttentionUNet(in_channels=6, out_channels=1).to(device)\n\n# # Loss Function\n# criterion = CombinedLoss()  # Binary Cross Entropy for segmentation\n\n# # Optimizer\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"execution":{"iopub.execute_input":"2025-02-19T13:52:56.817701Z","iopub.status.busy":"2025-02-19T13:52:56.817368Z","iopub.status.idle":"2025-02-19T13:52:56.821252Z","shell.execute_reply":"2025-02-19T13:52:56.820384Z","shell.execute_reply.started":"2025-02-19T13:52:56.817655Z"},"trusted":true},"outputs":[],"execution_count":139},{"cell_type":"code","source":"\n# def train(model, dataloader, optimizer, criterion, device):\n#     model.train()\n#     total_loss = 0\n#     for img, label in dataloader:\n#         img, label = img.to(device), label.to(device)\n        \n#         optimizer.zero_grad()\n#         output = model(img)\n        \n#         loss = criterion(output, label)\n#         loss.backward()\n#         optimizer.step()\n        \n#         total_loss += loss.item()\n    \n#     avg_loss = total_loss / len(dataloader)\n#     print(f\"Train Loss: {avg_loss:.4f}\")\n\n# def evaluate(model, dataloader, device):\n#     model.eval()\n#     correct = 0\n#     total = 0\n#     with torch.no_grad():\n#         for img, label in dataloader:\n#             img, label = img.to(device), label.to(device)\n#             output = model(img)\n#             predicted = (output > 0.5).float()\n#             correct += (predicted == label).sum().item()\n#             total += label.numel()\n    \n#     accuracy = 100 * correct / total\n#     print(f\"Accuracy: {accuracy:.2f}%\")\n\n# # Train the Model\n# epochs = 10\n# for epoch in range(epochs):\n#     print(f\"Epoch {epoch+1}/{epochs}\")\n#     train(model, train_loader, optimizer, criterion, device)\n#     evaluate(model, test_loader, device)\n","metadata":{"execution":{"iopub.execute_input":"2025-02-19T13:52:56.822442Z","iopub.status.busy":"2025-02-19T13:52:56.822085Z","iopub.status.idle":"2025-02-19T13:52:56.834408Z","shell.execute_reply":"2025-02-19T13:52:56.833596Z","shell.execute_reply.started":"2025-02-19T13:52:56.822387Z"},"trusted":true},"outputs":[],"execution_count":140},{"cell_type":"code","source":"# Import necessary modules\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport numpy as np\nimport tifffile as tiff\n\n# Define the device (use GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize your UNet model\nattention_unet = AttentionUNet(in_channels=6, out_channels=1)  # Adjust input channels for SAR, DEM, LULC, etc.\nattention_unet.to(device)\n\n\ndef dice_coef_metric(inputs, target):\n    intersection = 2.0 * (target * inputs).sum()\n    union = target.sum() + inputs.sum()\n    if target.sum() == 0 and inputs.sum() == 0:\n        return 1.0  # If both are empty, perfect match\n    return intersection / union\n\nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1.0):\n        # Apply sigmoid activation\n        inputs = torch.sigmoid(inputs)       \n        \n        # Flatten tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()\n        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n        \n        return 1 - dice  # Dice loss is (1 - dice coefficient)\n\ndef train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler=None, num_epochs=50):\n    print(f\"[INFO] Training Model: {model_name}\")\n    \n    loss_history = []\n    train_history = []\n    val_history = []\n    \n    bce_loss_fn = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss\n    \n    for epoch in range(num_epochs):\n        model.train()\n        \n        epoch_losses = []\n        epoch_dice = []\n        \n        for i_step, (data, target) in enumerate(tqdm(train_loader)):\n            data, target = data.to(device), target.to(device)\n            \n            outputs = model(data)\n            \n            # Convert outputs to binary mask using threshold\n            out_cut = torch.sigmoid(outputs)  # Apply sigmoid activation\n            out_cut = torch.where(out_cut < 0.5, torch.tensor(0.0, device=device), torch.tensor(1.0, device=device))\n            \n            # Compute Dice coefficient\n            train_dice = dice_coef_metric(out_cut, target)\n            \n            # Compute BCE + Dice Loss\n            bce_loss = bce_loss_fn(outputs, target)\n            dice_loss = train_loss(outputs, target)\n            loss = bce_loss + dice_loss  # Combined loss\n            \n            epoch_losses.append(loss.item())\n            epoch_dice.append(train_dice)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n        val_dice = compute_iou(model, val_loader)\n        \n        # Learning rate scheduling (if applicable)\n        if lr_scheduler:\n            lr_scheduler.step()\n        \n        loss_history.append(np.mean(epoch_losses))\n        train_history.append(np.mean([x.cpu().numpy() for x in epoch_dice]))\n        val_history.append(val_dice)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n        print(f\"Train Loss: {np.mean(epoch_losses):.4f}, Train Dice: {np.mean([x.cpu().numpy() for x in epoch_dice]):.4f}, Val Dice: {val_dice:.4f}\")\n\n    \n    return loss_history, train_history, val_history\n\ndef compute_iou(model, loader, threshold=0.5):\n    total_iou = 0\n    count = 0\n    \n    with torch.no_grad():\n        model.eval()\n\n        for i_step, (data, target) in enumerate(loader):\n            data, target = data.to(device), target.to(device)\n            \n            outputs = model(data)\n            out_cut = torch.sigmoid(outputs)\n            out_cut = torch.where(out_cut < threshold, torch.tensor(0.0, device=device), torch.tensor(1.0, device=device))\n            \n            iou = dice_coef_metric(out_cut, target)\n            total_iou += iou\n            count += 1\n\n    return total_iou / count if count > 0 else 0\n\n# Define optimizer\nopt = torch.optim.Adamax(attention_unet.parameters(), lr=1e-3)\n\n# Train Model\nnum_epochs = 5\naun_lh, aun_th, aun_vh = train_model(\"Attention UNet\", attention_unet, train_loader, test_loader, DiceLoss(), opt, None, num_epochs)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T11:48:03.092802Z","iopub.execute_input":"2025-03-19T11:48:03.093154Z","iopub.status.idle":"2025-03-19T11:48:13.923310Z","shell.execute_reply.started":"2025-03-19T11:48:03.093127Z","shell.execute_reply":"2025-03-19T11:48:13.922105Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[INFO] Training Model: Attention UNet\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 6/326 [00:10<09:06,  1.71s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-0fb563389994>\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0maun_lh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maun_th\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maun_vh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attention UNet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_unet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiceLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-0fb563389994>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"torch.save(attention_unet.state_dict(), \"flood_segmentation_model.pth\")\nprint(\"Model saved successfully!\")\n","metadata":{"execution":{"iopub.execute_input":"2025-02-19T15:20:07.262629Z","iopub.status.busy":"2025-02-19T15:20:07.262251Z","iopub.status.idle":"2025-02-19T15:20:07.611138Z","shell.execute_reply":"2025-02-19T15:20:07.610343Z","shell.execute_reply.started":"2025-02-19T15:20:07.262603Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved successfully!\n"]}],"execution_count":148},{"cell_type":"code","source":"# Load the trained model\nmodel = AttentionUNet(6, 1)  # Use the correct model class\nmodel.load_state_dict(torch.load(\"/kaggle/input/improved-unet/pytorch/default/1/flood_segmentation_model_uncertainity.pth\"))\nmodel.to(device)\nmodel.eval()  # Set to evaluation mode\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tifffile as tiff\nimport torch\nimport numpy as np\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define image path\nimage_path = \"/kaggle/input/images/images/images/4.tif\"\n\n# Load and preprocess the image using tifffile\nimage = tiff.imread(image_path)\n\n# Normalize and convert to tensor\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # Convert to tensor\n])\n\nimage = transform(image).float()  # Convert to float32\nimage = image.unsqueeze(0)  # Add batch dimension\nimage = image.to(device)\n\n# ✅ Ensure image is in float32\nprint(f\"Image dtype: {image.dtype}\")  # Should print `torch.float32`\n\n# Run inference\nmodel.eval()\nwith torch.no_grad():\n    output = model(image)\n\n# Convert output to binary mask\npred_mask = output.sigmoid().cpu().numpy().squeeze()\npred_mask = (pred_mask > 0.6).astype(np.uint8)  # Binary (0 or 1)\n\n# Load ground truth (assuming it's a PNG image)\nlabel_path = \"/kaggle/input/images/labels/labels/4.png\"\ngt_mask = plt.imread(label_path)\n\n# Normalize GT Mask to Binary (0 or 1)\ngt_mask = (gt_mask > 0.001).astype(np.uint8)\n\n\n# Visualize prediction vs ground truth\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nax[0].imshow(gt_mask, cmap=\"gray\")\nax[0].set_title(\"Ground Truth\")\nax[0].axis(\"off\")\n\nax[1].imshow(pred_mask, cmap=\"gray\")\nax[1].set_title(\"Predicted Mask\")\nax[1].axis(\"off\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-03-19T12:15:45.242101Z","iopub.execute_input":"2025-03-19T12:15:45.242447Z","iopub.status.idle":"2025-03-19T12:15:45.791209Z","shell.execute_reply.started":"2025-03-19T12:15:45.242420Z","shell.execute_reply":"2025-03-19T12:15:45.790124Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Image dtype: torch.float32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# ----------------------- Compute Training Accuracy -----------------------\ntrain_correct = 0\ntrain_total = 0\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        pred_masks = (outputs.sigmoid() > 0.51).float()\n\n        train_correct += (pred_masks == labels).sum().item()\n        train_total += labels.numel()\n\ntrain_accuracy = train_correct / train_total\nprint(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n\n# ---------------------- Evaluate Model on Test Data ----------------------\ntest_preds = []\ntest_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        pred_masks = (outputs.sigmoid() > 0.51).cpu().numpy().astype(np.uint8)\n        labels = labels.cpu().numpy().astype(np.uint8)\n\n        test_preds.extend(pred_masks.flatten())\n        test_labels.extend(labels.flatten())\n\n# Compute Accuracy & F1 Score\ntest_accuracy = accuracy_score(test_labels, test_preds)\ntest_f1 = f1_score(test_labels, test_preds)\n\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\nprint(f\"Test F1 Score: {test_f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T12:40:13.666376Z","iopub.execute_input":"2025-03-19T12:40:13.666738Z","iopub.status.idle":"2025-03-19T12:46:59.346936Z","shell.execute_reply.started":"2025-03-19T12:40:13.666712Z","shell.execute_reply":"2025-03-19T12:46:59.346094Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Train Accuracy: 97.33%\nTest Accuracy: 97.09%\nTest F1 Score: 0.7967\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"pred_mask, gt_mask","metadata":{"execution":{"iopub.status.busy":"2025-03-19T12:46:59.347959Z","iopub.execute_input":"2025-03-19T12:46:59.348272Z","iopub.status.idle":"2025-03-19T12:46:59.354545Z","shell.execute_reply.started":"2025-03-19T12:46:59.348248Z","shell.execute_reply":"2025-03-19T12:46:59.353658Z"},"trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(array([[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n array([[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8))"},"metadata":{}}],"execution_count":15}]}