{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10756922,"sourceType":"datasetVersion","datasetId":6672060}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install rasterio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:23:41.841507Z","iopub.execute_input":"2025-03-27T12:23:41.841955Z","iopub.status.idle":"2025-03-27T12:23:45.551368Z","shell.execute_reply.started":"2025-03-27T12:23:41.841915Z","shell.execute_reply":"2025-03-27T12:23:45.550462Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rasterio in /usr/local/lib/python3.10/dist-packages (1.4.3)\nRequirement already satisfied: affine in /usr/local/lib/python3.10/dist-packages (from rasterio) (2.4.0)\nRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (25.1.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2025.1.31)\nRequirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\nRequirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.26.4)\nRequirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from rasterio) (3.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->rasterio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->rasterio) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->rasterio) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom sklearn.model_selection import train_test_split\n\n# Paths\nimg_dir = \"/kaggle/input/images/images/images\"\nlabel_dir = \"/kaggle/input/images/labels/labels\"\n\n# List images and labels\nimg_list = sorted([f for f in os.listdir(img_dir) if f.endswith(\".tif\")])\nlabel_list = sorted([f for f in os.listdir(label_dir) if f.endswith(\".png\")])\n\n# Keep only matching pairs\nimg_paths = [os.path.join(img_dir, f) for f in img_list if f.replace(\".tif\", \".png\") in label_list]\nlabel_paths = [os.path.join(label_dir, f.replace(\".tif\", \".png\")) for f in img_list if f.replace(\".tif\", \".png\") in label_list]\n\n# Train-test split\ntrain_img_paths, test_img_paths, train_label_paths, test_label_paths = train_test_split(\n    img_paths, label_paths, test_size=0.2, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:23:45.552395Z","iopub.execute_input":"2025-03-27T12:23:45.552625Z","iopub.status.idle":"2025-03-27T12:23:46.104787Z","shell.execute_reply.started":"2025-03-27T12:23:45.552604Z","shell.execute_reply":"2025-03-27T12:23:46.104047Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import tifffile as tiff\n# import numpy as np\n# import cv2\n# import matplotlib.pyplot as plt\n# from PIL import Image  # For loading PNG ground truth\n\n# # **Step 1: Load the 6-Channel Image**\n# image_path = \"/kaggle/input/images/images/images/3.tif\"\n# label_path = \"/kaggle/input/images/labels/labels/3.png\"  # Update this path\n\n# image = tiff.imread(image_path).astype(np.float32)  # Shape: (H, W, C)\n# image = np.transpose(image, (2, 0, 1))  # Convert to (C, H, W)\n# assert image.shape[0] == 6, f\"Expected 6 channels, got {image.shape[0]}\"\n\n# # **Step 2: Load the Ground Truth PNG Label**\n# label = Image.open(label_path).convert(\"L\")  # Convert to grayscale\n# label = np.array(label)  # Convert to NumPy array\n\n# # **Step 3: Normalize Continuous Channels ([0,1])**\n# def normalize_channel(channel):\n#     return (channel - np.min(channel)) / (np.max(channel) - np.min(channel) + 1e-6)\n\n# # Normalize all channels except LULC (5th channel)\n# for i in [0, 1, 2, 3, 5]:  # Skip index 4 (LULC)\n#     image[i] = normalize_channel(image[i])\n\n# # **Step 4: Resize Low-Resolution Channels**\n# target_size = (image.shape[1], image.shape[2])  # Target resolution (H, W) = 10m\n# image[3] = cv2.resize(image[3], target_size, interpolation=cv2.INTER_CUBIC)  # DEM 30m\n# image[5] = cv2.resize(image[5], target_size, interpolation=cv2.INTER_CUBIC)  # Water Occurrence\n\n# # **Step 5: One-Hot Encode LULC (Categorical Channel)**\n# unique_classes = np.unique(image[4])  # Find unique class labels\n# num_classes = len(unique_classes)\n# print(f\"LULC Unique Classes: {unique_classes}\")\n\n# # Create empty one-hot encoded array\n# lulc_one_hot = np.zeros((num_classes, image.shape[1], image.shape[2]))\n\n# for i, class_value in enumerate(unique_classes):\n#     lulc_one_hot[i] = (image[4] == class_value).astype(np.float32)\n\n# # **Step 6: Combine Channels for CNN**\n# # Continuous Channels: [SAR1, SAR2, DEM1, DEM2, Water Occurrence]\n# continuous_channels = np.array([image[i] for i in [0, 1, 2, 3, 5]])\n# combined_input = np.concatenate((continuous_channels, lulc_one_hot), axis=0)  # Shape: (C, H, W)\n\n# # Convert to PyTorch Tensor\n# image_tensor = torch.tensor(combined_input, dtype=torch.float32).unsqueeze(0)  # Shape: (1, C, H, W)\n# print(\"Updated Input Image Shape:\", image_tensor.shape)  # Should be (1, C, H, W)\n\n# # **Step 7: Define CNN-Based Fusion Model**\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# class FusionCNN(nn.Module):\n#     def __init__(self, num_input_channels):\n#         super(FusionCNN, self).__init__()\n\n#         # First block: Extract low-level spatial features\n#         self.conv1 = nn.Conv2d(in_channels=num_input_channels, out_channels=64, kernel_size=3, padding=1)\n#         self.bn1 = nn.BatchNorm2d(64)  # Batch normalization\n#         self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n#         self.bn2 = nn.BatchNorm2d(64)\n\n#         # Second block: Extract mid-level features\n#         self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n#         self.bn3 = nn.BatchNorm2d(32)\n#         self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n#         self.bn4 = nn.BatchNorm2d(32)\n\n#         # Third block: Extract high-level semantic features\n#         self.conv5 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n#         self.bn5 = nn.BatchNorm2d(16)\n#         self.conv6 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n#         self.bn6 = nn.BatchNorm2d(16)\n\n#         # Final layer: Reduce to 1 feature map\n#         self.conv_final = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, padding=1)\n        \n#         # Dropout for regularization\n#         self.dropout = nn.Dropout(0.3)  \n\n#     def forward(self, x):\n#         # First block\n#         x = F.relu(self.bn1(self.conv1(x)))\n#         x = F.relu(self.bn2(self.conv2(x)))\n#         x = self.dropout(x)  # Apply dropout\n\n#         # Second block\n#         x = F.relu(self.bn3(self.conv3(x)))\n#         x = F.relu(self.bn4(self.conv4(x)))\n#         x = self.dropout(x)\n\n#         # Third block\n#         x = F.relu(self.bn5(self.conv5(x)))\n#         x = F.relu(self.bn6(self.conv6(x)))\n#         x = self.dropout(x)\n\n#         # Final output\n#         x = self.conv_final(x)\n#         return x.squeeze(0)  # Shape: (1, H, W)\n\n# # **Initialize the Model**\n# num_input_channels = combined_input.shape[0]  \n# print(num_input_channels)\n# fusion_model = FusionCNN(num_input_channels)\n\n# # **Perform Feature Fusion**\n# fused_feature_map = fusion_model(image_tensor)\n\n# # **Print Model Summary**\n# print(fusion_model)\n\n# # **Step 9: Visualize Fused Feature Map & Ground Truth**\n# fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# # **Plot CNN-Fused Feature Map**\n# axes[0].imshow(fused_feature_map.detach().numpy().squeeze(), cmap=\"viridis\")\n# axes[0].set_title(\"CNN-Based Fused Feature Map\")\n# axes[0].axis(\"off\")\n\n# # **Plot Ground Truth Label**\n# axes[1].imshow(label, cmap=\"gray\")\n# axes[1].set_title(\"Ground Truth (Label)\")\n# axes[1].axis(\"off\")\n\n# plt.show()\n\n# # **Step 10: Print Shape & Tensor**\n# print(\"Fused Feature Map Shape:\", fused_feature_map.shape)  # Should be (H, W)\n# print(\"Fused Feature Map Tensor:\", fused_feature_map)\n# print(\"Ground Truth Shape:\", label.shape)  # Should be (H, W)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:23:46.106327Z","iopub.execute_input":"2025-03-27T12:23:46.106763Z","iopub.status.idle":"2025-03-27T12:23:46.111591Z","shell.execute_reply.started":"2025-03-27T12:23:46.106727Z","shell.execute_reply":"2025-03-27T12:23:46.110737Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport numpy as np\nimport tifffile as tiff\nimport cv2\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\n\nclass FusionCNN(nn.Module):\n    def __init__(self, num_input_channels):\n        super(FusionCNN, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels=num_input_channels, out_channels=64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)  \n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(32)\n        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n        self.bn4 = nn.BatchNorm2d(32)\n\n        self.conv5 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.bn5 = nn.BatchNorm2d(16)\n        self.conv6 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n        self.bn6 = nn.BatchNorm2d(16)\n\n        self.conv_final = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, padding=1)\n        self.dropout = nn.Dropout(0.3)  \n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.dropout(x)  \n\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = F.relu(self.bn4(self.conv4(x)))\n        x = self.dropout(x)\n\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.relu(self.bn6(self.conv6(x)))\n        x = self.dropout(x)\n\n        x = self.conv_final(x)\n        return x.squeeze(0)  \n\n\nclass FusionDataset(Dataset):\n    def __init__(self, img_paths, label_paths, fusion_model, num_lulc_classes, device):\n        self.img_paths = img_paths\n        self.label_paths = label_paths\n        self.fusion_model = fusion_model.eval()  \n        self.num_lulc_classes = num_lulc_classes\n        self.device = device\n\n    def normalize_channel(self, channel):\n        return (channel - np.min(channel)) / (np.max(channel) - np.min(channel) + 1e-6)\n\n    def __getitem__(self, idx):\n        img = tiff.imread(self.img_paths[idx]).astype(np.float32)  \n        img = np.transpose(img, (2, 0, 1))  \n\n        label = Image.open(self.label_paths[idx]).convert(\"L\")  \n        label = np.array(label, dtype=np.float32)  \n\n        for i in [0, 1, 2, 3, 5]:  \n            img[i] = self.normalize_channel(img[i])\n\n        target_size = (img.shape[1], img.shape[2])  \n        img[3] = cv2.resize(img[3], target_size, interpolation=cv2.INTER_CUBIC)  \n        img[5] = cv2.resize(img[5], target_size, interpolation=cv2.INTER_CUBIC)  \n\n        lulc_one_hot = np.zeros((self.num_lulc_classes, img.shape[1], img.shape[2]))\n        for class_value in range(self.num_lulc_classes):\n            lulc_one_hot[class_value] = (img[4] == class_value).astype(np.float32)\n\n        continuous_channels = np.array([img[i] for i in [0, 1, 2, 3, 5]])  \n        combined_input = np.concatenate((continuous_channels, lulc_one_hot), axis=0)  \n\n        img_tensor = torch.tensor(combined_input, dtype=torch.float32).unsqueeze(0).to(self.device)  \n\n        with torch.no_grad():\n            fused_feature_map = self.fusion_model(img_tensor)  \n\n        label_tensor = torch.tensor(label, dtype=torch.float32).unsqueeze(0).to(self.device)  \n\n        return fused_feature_map, label_tensor\n\n    def __len__(self):\n        return len(self.img_paths)\n\n\ndevice = torch.device(\"cuda\")\nprint(f\"Using device: {device}\")\n\nimg_dir = \"/kaggle/input/images/images/images\"\nlabel_dir = \"/kaggle/input/images/labels/labels\"\n\nimg_list = sorted([f for f in os.listdir(img_dir) if f.endswith(\".tif\")])\nlabel_list = sorted([f for f in os.listdir(label_dir) if f.endswith(\".png\")])\n\nimg_paths = [os.path.join(img_dir, f) for f in img_list if f.replace(\".tif\", \".png\") in label_list]\nlabel_paths = [os.path.join(label_dir, f.replace(\".tif\", \".png\")) for f in img_list if f.replace(\".tif\", \".png\") in label_list]\n\ntrain_img_paths, test_img_paths, train_label_paths, test_label_paths = train_test_split(\n    img_paths, label_paths, test_size=0.2, random_state=42)\n\nlulc_img = tiff.imread(train_img_paths[0])[4]  \nnum_lulc_classes = len(np.unique(lulc_img))  \n\nnum_input_channels = 5 + num_lulc_classes  \nfusion_model = FusionCNN(num_input_channels).to(device)\n\ntrain_dataset = FusionDataset(train_img_paths, train_label_paths, fusion_model, num_lulc_classes, device)\ntest_dataset = FusionDataset(test_img_paths, test_label_paths, fusion_model, num_lulc_classes, device)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n\nfused_features, labels = next(iter(train_loader))\nprint(\"Fused Feature Map Shape:\", fused_features.shape)  \nprint(\"Label Shape:\", labels.shape)  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:23:46.112646Z","iopub.execute_input":"2025-03-27T12:23:46.112972Z","iopub.status.idle":"2025-03-27T12:24:00.872531Z","shell.execute_reply.started":"2025-03-27T12:23:46.112942Z","shell.execute_reply":"2025-03-27T12:24:00.871590Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nFused Feature Map Shape: torch.Size([4, 1, 512, 512])\nLabel Shape: torch.Size([4, 1, 512, 512])\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvBlock(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.conv(x)\n\nclass UpConvBlock(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super().__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.up(x)\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, f_g, f_l, f_int):\n        super().__init__()\n        self.w_g = nn.Sequential(\n            nn.Conv2d(f_g, f_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(f_int)\n        )\n        self.w_x = nn.Sequential(\n            nn.Conv2d(f_l, f_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(f_int)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(f_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, g, x):\n        g1 = self.w_g(g)\n        x1 = self.w_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        return x * psi\n\nclass AttentionUNet(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__() \n        \n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.conv1 = ConvBlock(ch_in=in_channels, ch_out=64)  # Updated to 11 channels\n        self.conv2 = ConvBlock(ch_in=64, ch_out=128)\n        self.conv3 = ConvBlock(ch_in=128, ch_out=256)\n        self.conv4 = ConvBlock(ch_in=256, ch_out=512)\n        self.conv5 = ConvBlock(ch_in=512, ch_out=1024)\n        \n        self.up5 = UpConvBlock(ch_in=1024, ch_out=512)\n        self.att5 = AttentionBlock(f_g=512, f_l=512, f_int=256)\n        self.upconv5 = ConvBlock(ch_in=1024, ch_out=512)\n        \n        self.up4 = UpConvBlock(ch_in=512, ch_out=256)\n        self.att4 = AttentionBlock(f_g=256, f_l=256, f_int=128)\n        self.upconv4 = ConvBlock(ch_in=512, ch_out=256)\n        \n        self.up3 = UpConvBlock(ch_in=256, ch_out=128)\n        self.att3 = AttentionBlock(f_g=128, f_l=128, f_int=64)\n        self.upconv3 = ConvBlock(ch_in=256, ch_out=128)\n        \n        self.up2 = UpConvBlock(ch_in=128, ch_out=64)\n        self.att2 = AttentionBlock(f_g=64, f_l=64, f_int=32)\n        self.upconv2 = ConvBlock(ch_in=128, ch_out=64)\n        \n        self.conv_1x1 = nn.Conv2d(64, out_channels, kernel_size=1, stride=1, padding=0)\n        \n    def forward(self, x):\n        # Encoder\n        x1 = self.conv1(x)\n        x2 = self.conv2(self.maxpool(x1))\n        x3 = self.conv3(self.maxpool(x2))\n        x4 = self.conv4(self.maxpool(x3))\n        x5 = self.conv5(self.maxpool(x4))\n        \n        # Decoder + Attention\n        d5 = self.upconv5(torch.cat((self.att5(self.up5(x5), x4), self.up5(x5)), dim=1))\n        d4 = self.upconv4(torch.cat((self.att4(self.up4(d5), x3), self.up4(d5)), dim=1))\n        d3 = self.upconv3(torch.cat((self.att3(self.up3(d4), x2), self.up3(d4)), dim=1))\n        d2 = self.upconv2(torch.cat((self.att2(self.up2(d3), x1), self.up2(d3)), dim=1))\n        \n        d1 = self.conv_1x1(d2)\n        \n        return torch.sigmoid(d1)  # Output probability map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:24:00.873358Z","iopub.execute_input":"2025-03-27T12:24:00.873733Z","iopub.status.idle":"2025-03-27T12:24:00.888117Z","shell.execute_reply.started":"2025-03-27T12:24:00.873708Z","shell.execute_reply":"2025-03-27T12:24:00.887134Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport numpy as np\n\ndevice = torch.device(\"cuda\")\n\nattention_unet = AttentionUNet(in_channels=1, out_channels=1)  # Adjust input channels for SAR, DEM, LULC, etc.\nattention_unet.to(device)\n\n\ndef dice_coef_metric(inputs, target):\n    intersection = 2.0 * (target * inputs).sum()\n    union = target.sum() + inputs.sum()\n    if target.sum() == 0 and inputs.sum() == 0:\n        return 1.0  # If both are empty, perfect match\n    return intersection / union\n\nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1.0):\n        # Apply sigmoid activation\n        inputs = torch.sigmoid(inputs)       \n        \n        # Flatten tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()\n        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n        \n        return 1 - dice  # Dice loss is (1 - dice coefficient)\n\ndef train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler=None, num_epochs=50):\n    print(f\"[INFO] Training Model: {model_name}\")\n    \n    loss_history = []\n    train_history = []\n    val_history = []\n    \n    bce_loss_fn = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss\n    \n    for epoch in range(num_epochs):\n        model.train()\n        \n        epoch_losses = []\n        epoch_dice = []\n        \n        for i_step, (data, target) in enumerate(tqdm(train_loader)):\n            data, target = data.to(device), target.to(device)\n            \n            outputs = model(data)\n            \n            # Convert outputs to binary mask using threshold\n            out_cut = torch.sigmoid(outputs)  # Apply sigmoid activation\n            out_cut = torch.where(out_cut < 0.5, torch.tensor(0.0, device=device), torch.tensor(1.0, device=device))\n            \n            # Compute Dice coefficient\n            train_dice = dice_coef_metric(out_cut, target)\n            \n            # Compute BCE + Dice Loss\n            bce_loss = bce_loss_fn(outputs, target)\n            dice_loss = train_loss(outputs, target)\n            loss = bce_loss + dice_loss  # Combined loss\n            \n            epoch_losses.append(loss.item())\n            epoch_dice.append(train_dice)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n        val_dice = compute_iou(model, val_loader)\n        \n        # Learning rate scheduling (if applicable)\n        if lr_scheduler:\n            lr_scheduler.step()\n        \n        loss_history.append(np.mean(epoch_losses))\n        train_history.append(np.mean([x.cpu().numpy() for x in epoch_dice]))\n        val_history.append(val_dice)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n        print(f\"Train Loss: {np.mean(epoch_losses):.4f}, Train Dice: {np.mean([x.cpu().numpy() for x in epoch_dice]):.4f}, Val Dice: {val_dice:.4f}\")\n\n    \n    return loss_history, train_history, val_history\n\ndef compute_iou(model, loader, threshold=0.5):\n    total_iou = 0\n    count = 0\n    \n    with torch.no_grad():\n        model.eval()\n\n        for i_step, (data, target) in enumerate(loader):\n            data, target = data.to(device), target.to(device)\n            \n            outputs = model(data)\n            out_cut = torch.sigmoid(outputs)\n            out_cut = torch.where(out_cut < threshold, torch.tensor(0.0, device=device), torch.tensor(1.0, device=device))\n            \n            iou = dice_coef_metric(out_cut, target)\n            total_iou += iou\n            count += 1\n\n    return total_iou / count if count > 0 else 0\n\n# Define optimizer\nopt = torch.optim.Adamax(attention_unet.parameters(), lr=1e-3)\n# Train Model\nnum_epochs = 1\naun_lh, aun_th, aun_vh = train_model(\"Attepresntion UNet\", attention_unet, train_loader, test_loader, DiceLoss(), opt, None, num_epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:24:00.889197Z","iopub.execute_input":"2025-03-27T12:24:00.889529Z","execution_failed":"2025-03-27T17:02:13.948Z"}},"outputs":[{"name":"stdout","text":"[INFO] Training Model: Attepresntion UNet\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 326/326 [1:04:25<00:00, 11.86s/it]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"torch.save(attention_unet.state_dict(), \"flood_segmentation_model.pth\")\nprint(\"Model saved successfully!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-27T17:02:13.950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AttentionUNet(1, 1)  # Use the correct model class\nmodel.load_state_dict(torch.load(\"flood_segmentation_model.pth\"))\nmodel.to(device)\nmodel.eval()  # Set to evaluation mode","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-27T17:02:13.950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tifffile as tiff\nimport torch\nimport numpy as np\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport cv2\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load FusionCNN and AttentionUNet models\nfusion_model = FusionCNN(num_input_channels=6 + num_lulc_classes).to(device)\nsegmentation_model = AttentionUNet(in_channels=1).to(device)\n\n# Set models to evaluation mode\nfusion_model.eval()\nsegmentation_model.eval()\n\n# Define image path\nimage_path = \"/kaggle/input/images/images/images/4.tif\"\n\n# Load and preprocess the image using tifffile\nimage = tiff.imread(image_path).astype(np.float32)  # Shape: (H, W, C)\nimage = np.transpose(image, (2, 0, 1))  # Convert to (C, H, W)\n\n# Normalize specific bands\ndef normalize_channel(channel):\n    return (channel - np.min(channel)) / (np.max(channel) - np.min(channel) + 1e-6)\n\nfor i in [0, 1, 2, 3, 5]:  # Normalize specific bands\n    image[i] = normalize_channel(image[i])\n\n# Resize LULC and DEM bands\ntarget_size = (image.shape[1], image.shape[2])  \nimage[3] = cv2.resize(image[3], target_size, interpolation=cv2.INTER_CUBIC)  # DEM\nimage[5] = cv2.resize(image[5], target_size, interpolation=cv2.INTER_CUBIC)  # LULC\n\n# One-hot encode LULC\nlulc_one_hot = np.zeros((num_lulc_classes, image.shape[1], image.shape[2]))\nfor class_value in range(num_lulc_classes):\n    lulc_one_hot[class_value] = (image[4] == class_value).astype(np.float32)\n\n# Combine continuous bands and one-hot LULC\ncombined_input = np.concatenate((image[[0, 1, 2, 3, 5]], lulc_one_hot), axis=0)\n\n# Convert to tensor and move to GPU\nimage_tensor = torch.tensor(combined_input, dtype=torch.float32).unsqueeze(0).to(device)\n\n# Run FusionCNN\nwith torch.no_grad():\n    fused_feature_map = fusion_model(image_tensor)  # Shape: (1, H, W)\n\n# Prepare for AttentionUNet\nfused_feature_map = fused_feature_map.unsqueeze(0)  # Add channel dimension -> (1, 1, H, W)\n\n# Run segmentation model\nwith torch.no_grad():\n    output = segmentation_model(fused_feature_map)  # Shape: (1, 1, H, W)\n\n# Convert output to binary mask\npred_mask = output.sigmoid().cpu().numpy().squeeze()\npred_mask = (pred_mask > 0.6).astype(np.uint8)  # Binary (0 or 1)\n\n# Load ground truth (assuming it's a PNG image)\nlabel_path = \"/kaggle/input/images/labels/labels/4.png\"\ngt_mask = plt.imread(label_path)\n\n# Normalize GT Mask to Binary (0 or 1)\ngt_mask = (gt_mask > 0.001).astype(np.uint8)\n\n# Visualize prediction vs ground truth\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nax[0].imshow(gt_mask, cmap=\"gray\")\nax[0].set_title(\"Ground Truth\")\nax[0].axis(\"off\")\n\nax[1].imshow(pred_mask, cmap=\"gray\")\nax[1].set_title(\"Predicted Mask\")\nax[1].axis(\"off\")\n\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-27T17:02:13.950Z"}},"outputs":[],"execution_count":null}]}